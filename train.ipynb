{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only using sensors data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config env and callback function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV CONF\n",
    "import carla \n",
    "# client = carla.Client(\"localhost\", 2000)\n",
    "# world = client.load_world('Town01')\n",
    "import gymnasium as gym\n",
    "from carla_env import CarEnv\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "class CARLA_G(gym.Env):\n",
    "    def __init__(self, ):\n",
    "        super(CARLA_G, self).__init__()\n",
    "        self.env = CarEnv()\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape = (2, ), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low = -np.inf, high = np.inf, shape=(16, ), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        [new_image, new_state], reward, done, info = self.env.step(action)\n",
    "        return new_state.astype(np.float32), reward, done, False, {}\n",
    "    \n",
    "    def reset(self, seed = None, options = {}):\n",
    "        image, state = self.env.reset()\n",
    "        return state.astype(np.float32), {}\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MyCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(MyCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if there are completed episodes\n",
    "        if len(self.model.ep_info_buffer) > 0:\n",
    "            info = self.model.ep_info_buffer[-1]\n",
    "            self.episode_rewards.append(info['r'])\n",
    "            self.episode_lengths.append(info['l'])\n",
    "\n",
    "            # Here we log the episodic rewards and lengths to TensorBoard\n",
    "            self.logger.record('episode_reward', np.mean(self.episode_rewards[-100:]))\n",
    "            self.logger.record('episode_length', np.mean(self.episode_lengths[-100:]))\n",
    "            self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "env = CARLA_G()\n",
    "town_name = 'Town01'\n",
    "env.env.town_name = town_name\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC, PPO, DDPG\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "import os\n",
    "from stable_baselines3.common.logger import configure\n",
    "log_dir = \"./logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "#model conf\n",
    "#---new model\n",
    "model = SAC(\"MlpPolicy\", env, verbose=1,buffer_size=1000000,tensorboard_log=log_dir)\n",
    "\n",
    "#---load model\n",
    "# if you countinue learning from saved model, uncomment this lines:\n",
    "# model = SAC.load(\"model name\", print_system_info=True)\n",
    "# model2 = SAC(\"MlpPolicy\", env, verbose=1)\n",
    "# model.env = model2.env\n",
    "# model.load_replay_buffer(\"./buffer_SAC_model_sesnor.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "# Pass a TensorBoard logger to the model\n",
    "new_logger = configure(log_dir, [\"stdout\", \"tensorboard\"])\n",
    "\n",
    "\n",
    "# Instantiate the callback\n",
    "my_callback = MyCallback()\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "for i in range(10000000):\n",
    "    try:\n",
    "        name = \"SAC_model_sesnor_run_\"+str(i+1)\n",
    "        print(name)\n",
    "        model.learn(total_timesteps=5000, log_interval=4,callback = my_callback)\n",
    "        model.save(name)\n",
    "        #saving realy buffer for next learning process\n",
    "        model.save_replay_buffer(\"buffer_SAC_model_sesnor.pkl\")\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        time.sleep(0.2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
