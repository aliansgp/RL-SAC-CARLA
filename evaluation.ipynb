{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla \n",
    "# client = carla.Client(\"localhost\", 2000)\n",
    "# world = client.load_world('Town01')\n",
    "import gymnasium as gym\n",
    "from carla_env import CarEnv\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import SAC, PPO, DDPG\n",
    "%matplotlib inline\n",
    "\n",
    "class CARLA_G(gym.Env):\n",
    "    def __init__(self, ):\n",
    "        super(CARLA_G, self).__init__()\n",
    "        self.env = CarEnv()\n",
    "        self.action_space = gym.spaces.Box(low=-1\n",
    "                                           , high=1\n",
    "                                           , shape = (2, )\n",
    "                                           , dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low = -np.inf\n",
    "                                                , high = np.inf\n",
    "                                                , shape=(16, )\n",
    "                                                , dtype=np.float32)\n",
    "        \n",
    "    def step(self, action):\n",
    "        [new_image, new_state], reward, done, info = self.env.step(action)\n",
    "        # plt.imshow(new_image)\n",
    "        # new_image = np.transpose(new_image,(2,0,1))\n",
    "        \n",
    "        \n",
    "        return new_state.astype(np.float32), reward, done, False, {}\n",
    "    \n",
    "    def reset(self, seed = None, options = {}):\n",
    "        image, state = self.env.reset()\n",
    "        # image = np.transpose(image,(2,0,1))\n",
    "        return state.astype(np.float32), {}\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "env = CARLA_G()\n",
    "town_name = 'Town01'\n",
    "env.env.town_name= town_name\n",
    "obs, info = env.reset()\n",
    "\n",
    "\n",
    "# Evaluate the agent\n",
    "episode_reward = 0\n",
    "model = SAC.load(\"./SAC_model_sesnor_run_96.zip\", print_system_info=True)\n",
    "for _ in range(10000000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if terminated or truncated or info.get(\"is_success\", False):\n",
    "        print(\"Reward:\", episode_reward, \"Success?\", info.get(\"is_success\", False))\n",
    "        episode_reward = 0.0\n",
    "        obs, info = env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV CONF\n",
    "import carla \n",
    "# client = carla.Client(\"localhost\", 2000)\n",
    "# world = client.load_world('Town01')\n",
    "import gymnasium as gym\n",
    "from carla_env import CarEnv\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class CARLA_G_camera(gym.Env):\n",
    "    def __init__(self, ):\n",
    "        super(CARLA_G_camera, self).__init__()\n",
    "        self.env = CarEnv()\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape = (2, ), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1,shape=(3,60,160), dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        [new_image, new_state], reward, done, info = self.env.step(action)\n",
    "        first_ch_new_image = np.moveaxis(new_image,-1,0)/255\n",
    "        return (first_ch_new_image.astype(np.float32)), reward, done, False, {}\n",
    "    \n",
    "    def reset(self, seed = None, options = {}):\n",
    "        image, state = self.env.reset()\n",
    "        first_ch_image = np.moveaxis(image,-1,0)/255\n",
    "\n",
    "        return first_ch_image.astype(np.float32), {}\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MyCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(MyCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if there are completed episodes\n",
    "        if len(self.model.ep_info_buffer) > 0:\n",
    "            info = self.model.ep_info_buffer[-1]\n",
    "            self.episode_rewards.append(info['r'])\n",
    "            self.episode_lengths.append(info['l'])\n",
    "\n",
    "            # Here we log the episodic rewards and lengths to TensorBoard\n",
    "            self.logger.record('episode_reward', np.mean(self.episode_rewards[-100:]))\n",
    "            self.logger.record('episode_length', np.mean(self.episode_lengths[-100:]))\n",
    "            self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "env = CARLA_G_camera()\n",
    "town_name = 'Town01'\n",
    "env.env.town_name = town_name\n",
    "env.reset()\n",
    "\n",
    "# Evaluate the agent\n",
    "episode_reward = 0\n",
    "model = SAC.load(\"./SAC_model_camera_run_117.zip\", print_system_info=True)\n",
    "for _ in range(10000000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if terminated or truncated or info.get(\"is_success\", False):\n",
    "        print(\"Reward:\", episode_reward, \"Success?\", info.get(\"is_success\", False))\n",
    "        episode_reward = 0.0\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensors and camera data (fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV CONF\n",
    "import carla \n",
    "# client = carla.Client(\"localhost\", 2000)\n",
    "# world = client.load_world('Town01')\n",
    "import gymnasium as gym\n",
    "from carla_env import CarEnv\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "class CARLA_G_fusion(gym.Env):\n",
    "    def __init__(self, ):\n",
    "        super(CARLA_G_fusion, self).__init__()\n",
    "        self.env = CarEnv()\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape = (2, ), dtype=np.float32)\n",
    "        # self.observation_space = gym.spaces.Box(low = -np.inf, high = np.inf, shape=(16, ), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Dict({\"image\": gym.spaces.Box(low=0, high=1,shape=(3,60,160), dtype=np.float32), \"tracking\": gym.spaces.Box(low = -np.inf, high = np.inf, shape=(16, ), dtype=np.float32)})\n",
    "\n",
    "    def step(self, action):\n",
    "        [new_image, new_state], reward, done, info = self.env.step(action)\n",
    "        first_ch_new_image = np.moveaxis(new_image,-1,0)/255\n",
    "        return {\"image\":first_ch_new_image.astype(np.float32), \"tracking\":new_state.astype(np.float32)}, reward, done, False, {}\n",
    "    \n",
    "    def reset(self, seed = None, options = {}):\n",
    "        image, state = self.env.reset()\n",
    "        first_ch_image = np.moveaxis(image,-1,0)/255\n",
    "\n",
    "        return {\"image\":first_ch_image.astype(np.float32),\"tracking\":state.astype(np.float32)}, {}\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MyCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(MyCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if there are completed episodes\n",
    "        if len(self.model.ep_info_buffer) > 0:\n",
    "            info = self.model.ep_info_buffer[-1]\n",
    "            self.episode_rewards.append(info['r'])\n",
    "            self.episode_lengths.append(info['l'])\n",
    "\n",
    "            # Here we log the episodic rewards and lengths to TensorBoard\n",
    "            self.logger.record('episode_reward', np.mean(self.episode_rewards[-100:]))\n",
    "            self.logger.record('episode_length', np.mean(self.episode_lengths[-100:]))\n",
    "            self.logger.dump(step=self.num_timesteps)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "env = CARLA_G_fusion()\n",
    "town_name = 'Town01'\n",
    "env.env.town_name = town_name\n",
    "env.reset()\n",
    "\n",
    "# Evaluate the agent\n",
    "episode_reward = 0\n",
    "model = SAC.load(\"./SAC_model_fusion_run_106.zip\", print_system_info=True)\n",
    "for _ in range(10000000):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if terminated or truncated or info.get(\"is_success\", False):\n",
    "        print(\"Reward:\", episode_reward, \"Success?\", info.get(\"is_success\", False))\n",
    "        episode_reward = 0.0\n",
    "        obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
